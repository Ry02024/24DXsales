{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWLfsHIqRk/lRCILdLCBHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ry02024/24DXsales/blob/main/Preprocessing4TimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 全体のデータ処理フローと各ステップの詳細\n",
        "\n",
        "## 1. データの読み込み\n",
        "\n",
        "**処理内容:**\n",
        "- 生データを読み込み、初期のデータフレーム `join_data_df` を作成。\n",
        "- 読み込むデータには以下の列が含まれます:\n",
        "  - 日付 (`date`)\n",
        "  - 店舗ID (`store_id`)\n",
        "  - 商品ID (`product_id`)\n",
        "  - 商品価格 (`product_price`)\n",
        "  - 売上個数 (`product_num`)\n",
        "  - 商品カテゴリID (`category_id`)\n",
        "  - 商品カテゴリ名 (`category_name`)\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数        | 列数 |\n",
        "|-------------|------|\n",
        "| 1,119,570 行 | 7 列 |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. データの前処理\n",
        "\n",
        "**処理内容:**\n",
        "- `join_data_df` に対して、年 (`year`)、月 (`month`)、月番号 (`month_num`) の特徴量を追加し、データを拡張。\n",
        "- この処理により、新しいデータフレーム `join_data_df3` が生成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数         | 列数  |\n",
        "|--------------|-------|\n",
        "| 1,089,695 行 | 10 列 |\n",
        "|（データのクレンジングやフィルタリングにより減少） |（元の7列に `year`, `month`, `month_num` を追加） |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 特徴量の生成\n",
        "\n",
        "**処理内容:**\n",
        "- 商品ごとに月別の価格と売上個数を集計し、各月に対応する特徴量を作成。\n",
        "- 特徴量は以下のように生成されます:\n",
        "  - `product_price_1` 〜 `product_price_22`: 各月の商品価格\n",
        "  - `product_num_1` 〜 `product_num_22`: 各月の売上個数\n",
        "- これにより、データフレーム `join_data_df6` が作成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数      | 列数  |\n",
        "|-----------|-------|\n",
        "| 107,115 行 | 48 列 |\n",
        "|（基本情報 + 22ヶ月分の価格と売上個数） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. カタログの補完\n",
        "\n",
        "**処理内容:**\n",
        "- 全店舗に対して全商品の組み合わせを揃えることで、存在しない組み合わせのデータを補完。\n",
        "- これにより、欠損データが増加しますが、モデルの一貫性を保つために必要なステップです。\n",
        "- 結果として、データフレーム `join_data_df7` が生成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数       | 列数  |\n",
        "|------------|-------|\n",
        "| 162,720 行 | 48 列 |\n",
        "|（全店舗・全商品の組み合わせによる増加） |（`join_data_df6` と同等） |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 欠損値の補完\n",
        "\n",
        "**処理内容:**\n",
        "- `join_data_df7` に対して、欠損値の補完を実施。\n",
        "- 具体的には、`main_flag` の追加や価格・売上の補完を行います。\n",
        "- 補完方法には、前月の値の引き継ぎや統計値による補完が含まれます。\n",
        "- 補完後のデータフレーム `join_data_df9` が作成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数       | 列数  |\n",
        "|------------|-------|\n",
        "| 162,720 行 | 49 列 |\n",
        "|（行数は変化なし） |（`main_flag` の追加） |\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 追加の特徴量の作成\n",
        "\n",
        "**処理内容:**\n",
        "- `join_data_df9` に対して、さらに詳細な特徴量を作成。\n",
        "- 例として、平均値 (`ave_num`, `ave_price`)、複合特徴量 (`product_ave_num`, `category_ave_price` など) を追加。\n",
        "- これにより、データフレーム `join_data_df10` が生成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数       | 列数  |\n",
        "|------------|-------|\n",
        "| 162,720 行 | 63 列 |\n",
        "|（新たな特徴量を追加） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. スライディングウィンドウデータセットの生成\n",
        "\n",
        "**処理内容:**\n",
        "- `join_data_df10` を基に、スライディングウィンドウ手法を用いて時系列データセットを生成。\n",
        "- 過去の一定期間のデータを用いて現在の予測を行うためのデータセットを作成。\n",
        "- 訓練データ (`train_df`) とテストデータ (`test_df_prepared`) に分割。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| データタイプ | 行数               | 列数  |\n",
        "|--------------|--------------------|-------|\n",
        "| 訓練データ   | 数十万行（具体的な数値は提供されていません） | 63 列 |\n",
        "| テストデータ | 数万行（具体的な数値は提供されていません） | 63 列 |\n",
        "|（スライディングウィンドウによる特徴量の追加は行わない場合） | | |\n",
        "\n",
        "---\n",
        "\n",
        "## 8. トレンド特徴量の生成\n",
        "\n",
        "**処理内容:**\n",
        "- 売上や価格のトレンドを捉えるための特徴量を作成。\n",
        "- 例として、移動平均、指数平滑移動平均（EWMA）、トレンドの勾配などを計算。\n",
        "- これにより、データフレーム `join_data_df11` が生成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数        | 列数  |\n",
        "|-------------|-------|\n",
        "| 1,789,920 行 | 55 列 |\n",
        "|（トレンド特徴量の追加） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 9. カレンダー情報の追加\n",
        "\n",
        "**処理内容:**\n",
        "- 休日や祝日などのカレンダー情報をデータに統合。\n",
        "- `day_each_month`、`holiday_each_month`、`day_holiday_month` などのカレンダー関連の特徴量を追加。\n",
        "- これにより、データフレーム `predict_x_df` が作成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数   | 列数  |\n",
        "|--------|-------|\n",
        "| 3,060 行 | 40 列 |\n",
        "|（カレンダー情報の統合により減少または選択された特徴量のみを保持） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 10. グループ化された特徴量の生成\n",
        "\n",
        "**処理内容:**\n",
        "- データを店舗やカテゴリ、商品ごとにグループ化し、グループ単位の統計量（平均、中央値、最大値、最小値など）を計算。\n",
        "- 例として、各店舗の月別売上平均やカテゴリごとの価格中央値などを追加。\n",
        "- この処理により、さらに豊富な特徴量が生成されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数        | 列数  |\n",
        "|-------------|-------|\n",
        "| 1,789,920 行 | 55 列 |\n",
        "|（グループ化された特徴量の追加） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 11. 差分特徴量の生成\n",
        "\n",
        "**処理内容:**\n",
        "- 前月との売上や価格の差分を計算し、新たな特徴量として追加。\n",
        "- 例として、`diff_10_9_num`（10月と9月の売上差分）、`diff_10_9_price`（10月と9月の価格差分）など。\n",
        "- これにより、データフレーム `join_data_df11` が強化されます。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数        | 列数  |\n",
        "|-------------|-------|\n",
        "| 1,789,920 行 | 55 列 |\n",
        "|（差分特徴量の追加） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 12. 特徴量エンジニアリングの統合\n",
        "\n",
        "**処理内容:**\n",
        "- これまでに生成した全ての特徴量を統合し、最終的な特徴量セットを構築。\n",
        "- 重複する特徴量の削除や、不要な特徴量の除去を行い、モデルに最適な形に整理。\n",
        "- データフレーム `month_target_12_df` および `month_target_1_11_df` を作成。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| データフレーム名           | 行数        | 列数 |\n",
        "|----------------------------|-------------|------|\n",
        "| `month_target_12_df`       | 1,789,920 行 | 60 列 |\n",
        "| `month_target_1_11_df`     | 3,060 行     | 60 列 |\n",
        "\n",
        "---\n",
        "\n",
        "## 13. 売上上昇傾向フラグの作成\n",
        "\n",
        "**処理内容:**\n",
        "- 売上が上昇しているかどうかを示すフラグ (`up_num_flag`) を作成。\n",
        "- 例として、売上が前月よりも増加している場合にフラグを1、そうでない場合を0とする。\n",
        "- このフラグを用いることで、モデルに対して売上の上昇傾向を明示的に伝えることが可能。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| 行数        | 列数  |\n",
        "|-------------|-------|\n",
        "| 1,789,920 行 | 60 列 |\n",
        "|（`up_num_flag` の追加） | |\n",
        "\n",
        "---\n",
        "\n",
        "## 14. データの分割とテストデータのソート\n",
        "\n",
        "**処理内容:**\n",
        "- データセットを訓練用 (`train_df`) とテスト用 (`test_df`) に分割。\n",
        "- テストデータは時系列に基づきソートされ、モデルの評価に適した形に整形。\n",
        "- 最終的な予測用データフレーム `predict_x_df` を準備。\n",
        "\n",
        "**データ数と特徴量数:**\n",
        "\n",
        "| データタイプ     | 行数                     | 列数      |\n",
        "|------------------|--------------------------|-----------|\n",
        "| 訓練データ (`train_df`) | 約1,785,660 行           | 60 列     |\n",
        "| テストデータ (`test_df`) | 3,060 行                 | 40 列     |\n",
        "|（必要な特徴量のみを保持） |                        |           |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5qAEDxgGsOvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モジュール"
      ],
      "metadata": {
        "id": "x2Jr3G3p7Qf0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnPROi3g7X4W"
      },
      "source": [
        "!pip install jpholiday -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb40ee04-9bb6-45a4-b66f-20e6e228c406",
        "id": "FsVdNk7_7X4W"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.autonotebook import tqdm\n",
        "import calendar\n",
        "from datetime import date, timedelta, datetime\n",
        "import jpholiday"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-54460f80d7f7>:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Google Driveにマウント（接続）する。これにより、Colab上でGoogle Drive内のファイルにアクセスできるようになる。\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKbCIOYGCDTL",
        "outputId": "0fc926da-18f8-47eb-ea29-8c0d0c05fce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/マナビDX/PBL01/演習03（機械学習）/DXQuest_PBL01/Data/\""
      ],
      "metadata": {
        "id": "M_LflO5l-GJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.autonotebook import tqdm\n",
        "import calendar\n",
        "from datetime import date, timedelta, datetime\n",
        "import jpholiday\n",
        "# データ読み込み関数\n",
        "def load_data(data_dir):\n",
        "    print(\"データを読み込んでいます...\")\n",
        "    sales_history_df = pd.read_csv(data_dir + 'sales_history.csv')\n",
        "    item_categories_df = pd.read_csv(data_dir + 'item_categories.csv')\n",
        "    category_names_df = pd.read_csv(data_dir + 'category_names.csv')\n",
        "    test_df = pd.read_csv(data_dir + 'test.csv', index_col=0)\n",
        "    print(\"データの読み込みが完了しました。\")\n",
        "    return sales_history_df, item_categories_df, category_names_df, test_df\n",
        "\n",
        "# データ前処理関数\n",
        "def preprocess_data(sales_history_df, item_categories_df, category_names_df):\n",
        "    print(\"データの前処理を開始します...\")\n",
        "    join_data_df = pd.merge(sales_history_df, item_categories_df, on='商品ID', how='left')\n",
        "    join_data_df = pd.merge(join_data_df, category_names_df, on='商品カテゴリID', how='left')\n",
        "    join_data_df = join_data_df.drop_duplicates()\n",
        "\n",
        "    # カラム名変更\n",
        "    join_data_df = join_data_df.rename(columns={'日付': 'date', '店舗ID': 'store_id', '商品ID': 'product_id',\n",
        "                                                '商品価格': 'product_price', '売上個数': 'product_num',\n",
        "                                                '商品カテゴリID': 'category_id', '商品カテゴリ名': 'category_name'})\n",
        "\n",
        "    # dateをdatetime型に変換し、year, month, month_numを作成\n",
        "    join_data_df['date'] = pd.to_datetime(join_data_df['date'])\n",
        "    join_data_df['year'] = join_data_df['date'].dt.year\n",
        "    join_data_df['month'] = join_data_df['date'].dt.month\n",
        "    join_data_df['month_num'] = join_data_df['month']\n",
        "    join_data_df.loc[join_data_df['year'] == 2019, 'month_num'] += 12\n",
        "    join_data_df = join_data_df.drop(['date', 'year', 'month'], axis=1)\n",
        "\n",
        "    print(\"データの前処理が完了しました。\")\n",
        "    return join_data_df\n",
        "\n",
        "# 特徴量生成関数\n",
        "def generate_features(join_data_df):\n",
        "    print(\"特徴量を生成しています...\")\n",
        "    join_data_df.drop('category_name', axis=1, inplace=True, errors='ignore')\n",
        "    join_data_df4_1 = join_data_df.groupby(['product_id', 'store_id', 'category_id', 'month_num']).mean()\n",
        "    join_data_df4_2 = join_data_df.groupby(['product_id', 'store_id', 'category_id', 'month_num']).sum()\n",
        "    join_data_df4_2['product_price'] = join_data_df4_1['product_price']\n",
        "\n",
        "    join_data_df5 = join_data_df4_2.unstack(level=3)\n",
        "    join_data_df5.to_csv('./join_data_df5_main.csv')\n",
        "    join_data_df6 = pd.read_csv('./join_data_df5_main.csv', header=[0, 1, 2])\n",
        "\n",
        "    columns_1 = ['product_id', 'store_id', 'category_id']\n",
        "    columns_2 = ['product_price_' + str(i) for i in range(1, 23)]\n",
        "    columns_3 = ['product_num_' + str(i) for i in range(1, 23)]\n",
        "    join_data_df6.columns = columns_1 + columns_2 + columns_3\n",
        "\n",
        "    print(\"特徴量の生成が完了しました。\")\n",
        "    return join_data_df6\n",
        "\n",
        "def complete_catalog(join_data_df):\n",
        "    product_id_pd = join_data_df[['product_id', 'category_id']].drop_duplicates().sort_values('product_id').copy()\n",
        "    join_data_df7 = pd.DataFrame()\n",
        "\n",
        "    for store_id in tqdm(sorted(join_data_df['store_id'].unique())):\n",
        "        store_data = join_data_df.loc[join_data_df['store_id'] == store_id]\n",
        "        merged_data = pd.merge(store_data, product_id_pd, on='product_id', how='right')\n",
        "        merged_data['category_id_x'] = merged_data['category_id_y']\n",
        "        merged_data.rename(columns={'category_id_x':'category_id'}, inplace=True)\n",
        "        merged_data.drop(['category_id_y'], axis=1, inplace=True, errors='ignore')\n",
        "        merged_data['store_id'] = store_id\n",
        "        join_data_df7 = pd.concat([join_data_df7, merged_data])\n",
        "    return join_data_df7\n",
        "\n",
        "def fill_missing_values(join_data_df, test_df): #適切に平均値で保管できていない可能性\n",
        "    print(\"欠損値を補完しています...\")\n",
        "\n",
        "    # 'product_id'ごとに、product_price_1 から product_price_22 の欠損値をその商品の平均値で補完\n",
        "    price_columns = [f'product_price_{i}' for i in range(1, 23)]\n",
        "    join_data_df[price_columns] = join_data_df.groupby('product_id')[price_columns].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "    # product_price_1 から product_num_22 までの欠損値を0で補完\n",
        "    price_and_num_columns = [f'product_price_{i}' for i in range(1, 23)] + [f'product_num_{i}' for i in range(1, 23)]\n",
        "    join_data_df[price_and_num_columns] = join_data_df[price_and_num_columns].fillna(0)\n",
        "\n",
        "    # 各列の負の値を0にする\n",
        "    join_data_df[price_and_num_columns] = join_data_df[price_and_num_columns].clip(lower=0)\n",
        "\n",
        "    # main_flag列をすべて0で初期化\n",
        "    join_data_df['main_flag'] = 0\n",
        "\n",
        "    # test_df['商品ID']に含まれるproduct_idに対してmain_flagを1に更新\n",
        "    join_data_df.loc[join_data_df['product_id'].isin(test_df['商品ID']), 'main_flag'] = 1\n",
        "\n",
        "    print(\"欠損値の補完が完了しました。\")\n",
        "    return join_data_df\n",
        "\n",
        "# 特徴量作成関数\n",
        "def fill_features(join_data_df10):\n",
        "    print(\"追加の特徴量を生成しています...\")\n",
        "    # 特徴量生成1: 商品、カテゴリ、店舗ごとの平均値を生成\n",
        "    target_columns = ['product_id', 'store_id', 'category_id'] + [f'product_num_{i}' for i in range(1, 23)] + [f'product_price_{i}' for i in range(1, 23)]\n",
        "    join_data_df10_feats = join_data_df10[target_columns]\n",
        "\n",
        "    # 1. 商品ごとの平均個数と平均価格\n",
        "    product_ave_num = join_data_df10_feats.groupby('product_id').mean().loc[:, 'product_num_1': 'product_num_22'].mean(axis=1)\n",
        "    product_ave_price = join_data_df10_feats.groupby('product_id').mean().loc[:, 'product_price_1': 'product_price_22'].mean(axis=1)\n",
        "\n",
        "    join_data_df10.insert(3, 'product_ave_num', 0)\n",
        "    join_data_df10.insert(4, 'product_ave_price', 0)\n",
        "    for product_id in tqdm(product_ave_num.index):\n",
        "        join_data_df10.loc[join_data_df10['product_id'] == product_id, 'product_ave_num'] = product_ave_num[product_id]\n",
        "        join_data_df10.loc[join_data_df10['product_id'] == product_id, 'product_ave_price'] = product_ave_price[product_id]\n",
        "\n",
        "    # 2. カテゴリごとの平均個数と平均価格\n",
        "    category_ave_num = join_data_df10.groupby('category_id').mean().loc[:, 'product_num_1': 'product_num_22'].mean(axis=1)\n",
        "    category_ave_price = join_data_df10.groupby('category_id').mean().loc[:, 'product_price_1': 'product_price_22'].mean(axis=1)\n",
        "\n",
        "    join_data_df10.insert(5, 'category_ave_num', 0)\n",
        "    join_data_df10.insert(6, 'category_ave_price', 0)\n",
        "    for category_id in tqdm(category_ave_num.index):\n",
        "        join_data_df10.loc[join_data_df10['category_id'] == category_id, 'category_ave_num'] = category_ave_num[category_id]\n",
        "        join_data_df10.loc[join_data_df10['category_id'] == category_id, 'category_ave_price'] = category_ave_price[category_id]\n",
        "\n",
        "    # 3. 店舗ごとの平均個数と平均価格\n",
        "    store_ave_num = join_data_df10.groupby('store_id').mean().loc[:, 'product_num_1': 'product_num_22'].mean(axis=1)\n",
        "    store_ave_price = join_data_df10.groupby('store_id').mean().loc[:, 'product_price_1': 'product_price_22'].mean(axis=1)\n",
        "\n",
        "    join_data_df10.insert(7, 'store_ave_num', 0)\n",
        "    join_data_df10.insert(8, 'store_ave_price', 0)\n",
        "    for store_id in tqdm(store_ave_num.index):\n",
        "        join_data_df10.loc[join_data_df10['store_id'] == store_id, 'store_ave_num'] = store_ave_num[store_id]\n",
        "        join_data_df10.loc[join_data_df10['store_id'] == store_id, 'store_ave_price'] = store_ave_price[store_id]\n",
        "\n",
        "    # 特徴量生成2: 商品、カテゴリ、店舗ごとの組み合わせを生成\n",
        "    join_data_df10.insert(9, 'p_c_nun', join_data_df10['product_ave_num'] * join_data_df10['category_ave_num'])\n",
        "    join_data_df10.insert(10, 'p_s_nun', join_data_df10['product_ave_num'] * join_data_df10['store_ave_num'])\n",
        "    join_data_df10.insert(11, 'c_s_nun', join_data_df10['category_ave_num'] * join_data_df10['store_ave_num'])\n",
        "    join_data_df10.insert(12, 'p_c_price', join_data_df10['product_ave_price'] * join_data_df10['category_ave_price'])\n",
        "    join_data_df10.insert(13, 'p_s_price', join_data_df10['product_ave_price'] * join_data_df10['store_ave_price'])\n",
        "    join_data_df10.insert(14, 'c_s_price', join_data_df10['category_ave_price'] * join_data_df10['store_ave_price'])\n",
        "\n",
        "    # さらに複合特徴量を生成\n",
        "    join_data_df10.insert(15, 'p_c_s_nun', join_data_df10['product_ave_num'] * join_data_df10['category_ave_num'] * join_data_df10['store_ave_num'])\n",
        "    join_data_df10.insert(16, 'p_c_s_price', join_data_df10['product_ave_price'] * join_data_df10['category_ave_price'] * join_data_df10['store_ave_price'])\n",
        "\n",
        "    print(\"追加の特徴量生成が完了しました。\")\n",
        "    return join_data_df10\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_sliding_window_datasets(df,\n",
        "                                     columns_to_front=None,\n",
        "                                     window_size=12,\n",
        "                                     n_steps=11):\n",
        "    \"\"\"\n",
        "    スライディングウィンドウを用いて訓練データフレームとテストデータフレームを生成する関数。\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): 元のデータフレーム。\n",
        "    - columns_to_front (list, optional): 先頭に配置するカラムのリスト。デフォルトは以下のリスト。\n",
        "    - window_size (int, optional): ウィンドウのサイズ（月数）。デフォルトは12。\n",
        "    - n_steps (int, optional): ウィンドウを適用するステップ数。デフォルトは11。\n",
        "\n",
        "    Returns:\n",
        "    - train_df (pd.DataFrame): スライディングウィンドウを適用した訓練データフレーム。\n",
        "    - test_df (pd.DataFrame): スライディングウィンドウを適用したテストデータフレーム。\n",
        "    \"\"\"\n",
        "\n",
        "    # デフォルトの columns_to_front を設定\n",
        "    if columns_to_front is None:\n",
        "        columns_to_front = [\n",
        "            'main_flag', 'product_id', 'store_id', 'category_id',\n",
        "            'product_ave_num', 'product_ave_price',\n",
        "            'category_ave_num', 'category_ave_price',\n",
        "            'store_ave_num', 'store_ave_price',\n",
        "            'p_c_nun', 'p_s_nun', 'c_s_nun',\n",
        "            'p_c_price', 'p_s_price', 'c_s_price',\n",
        "            'p_c_s_nun', 'p_c_s_price'\n",
        "        ]\n",
        "\n",
        "    # 2. その他のカラムを取得（移動させたいカラムを除く）\n",
        "    remaining_columns = [col for col in df.columns if col not in columns_to_front]\n",
        "\n",
        "    # 3. 新しいカラム順序を作成\n",
        "    new_column_order = columns_to_front + remaining_columns\n",
        "\n",
        "    # データフレームを新しいカラム順に並べ替え\n",
        "    df_reordered = df[new_column_order].copy()\n",
        "\n",
        "    # テストデータの作成\n",
        "    # main_flagが1の行を抽出\n",
        "    tmp0_df = df_reordered.loc[df_reordered['main_flag'] == 1]\n",
        "\n",
        "    # test_dfの作成\n",
        "    tmp1_df = tmp0_df.loc[:, 'main_flag':'p_c_s_price'].copy()\n",
        "    tmp2_df = tmp0_df.loc[:, 'product_num_13':'product_num_22'].copy()\n",
        "    tmp2_df = tmp2_df.rename(columns=lambda x: x[:12] + str(int(x[12:])-12))\n",
        "    tmp3_df = tmp0_df.loc[:, 'product_price_13':'product_price_22'].copy()\n",
        "    tmp3_df = tmp3_df.rename(columns=lambda x: x[:14] + str(int(x[14:])-12))\n",
        "    tmp4_df = pd.concat([tmp1_df, tmp2_df, tmp3_df], axis=1)\n",
        "\n",
        "    # month_target を追加\n",
        "    tmp4_df.insert(1, 'month_target', 12)\n",
        "\n",
        "    test_df = tmp4_df.copy()\n",
        "\n",
        "    # 訓練データの作成\n",
        "    # main_flagが1以外の全行を対象とする\n",
        "    catarog_copy2 = df_reordered.copy()\n",
        "\n",
        "    # 予測対象の月を示す変数　month_targetを計算する関数\n",
        "    def calc_month_target(n):\n",
        "        tmp = n % 12\n",
        "        return 12 if tmp == 0 else tmp\n",
        "\n",
        "    # 空のデータフレームを用意\n",
        "    train_df = pd.DataFrame()\n",
        "\n",
        "    # スライディングウィンドウの適用\n",
        "    for i in tqdm(range(n_steps), desc=\"スライディングウィンドウ適用中\"):\n",
        "        # 基本カラムを抽出\n",
        "        tmp1_df = catarog_copy2.loc[:, 'main_flag':'p_c_s_price'].copy()\n",
        "\n",
        "        # product_numのシフト\n",
        "        product_num_cols = [f'product_num_{j}' for j in range(i+1, i+window_size+1)]\n",
        "        tmp_num = catarog_copy2[product_num_cols].copy()\n",
        "        tmp_num.columns = [f'product_num_{j-i}' for j in range(i+1, i+window_size+1)]\n",
        "\n",
        "        # product_priceのシフト\n",
        "        product_price_cols = [f'product_price_{j}' for j in range(i+1, i+window_size+1)]\n",
        "        tmp_price = catarog_copy2[product_price_cols].copy()\n",
        "        tmp_price.columns = [f'product_price_{j-i}' for j in range(i+1, i+window_size+1)]\n",
        "\n",
        "        # シフトしたデータを結合\n",
        "        tmp_combined = pd.concat([tmp1_df, tmp_num, tmp_price], axis=1)\n",
        "\n",
        "        # 'month_target' を追加\n",
        "        tmp_combined.insert(1, 'month_target', calc_month_target(i))\n",
        "\n",
        "        # 訓練データに追加\n",
        "        train_df = pd.concat([train_df, tmp_combined], ignore_index=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def generate_trend_features(train_df, test_df):\n",
        "    \"\"\"\n",
        "    訓練データとテストデータに対して、定義された特徴量操作を適用して新しい特徴量を生成する関数。\n",
        "\n",
        "    Parameters:\n",
        "    - train_df (pd.DataFrame): 訓練データフレーム。\n",
        "    - test_df (pd.DataFrame): テストデータフレーム。\n",
        "\n",
        "    Returns:\n",
        "    - train_df_gen (pd.DataFrame): 新しい特徴量が追加された訓練データフレーム。\n",
        "    - test_df_gen (pd.DataFrame): 新しい特徴量が追加されたテストデータフレーム。\n",
        "    \"\"\"\n",
        "\n",
        "    # 特徴量名と対応する計算方法を定義\n",
        "    feature_operations = {\n",
        "        'ave_num': lambda df: df.loc[:, 'product_num_1':'product_num_10'].mean(axis=1),\n",
        "        'ave_price': lambda df: df.loc[:, 'product_price_1':'product_price_10'].mean(axis=1),\n",
        "        'diff_10_9_num': lambda df: df['product_num_10'] - df['product_num_9'],\n",
        "        'diff_10_9_price': lambda df: df['product_price_10'] - df['product_price_9'],\n",
        "        'diff_10_1_num': lambda df: df['product_num_10'] - df['product_num_1'],\n",
        "        'diff_10_1_price': lambda df: df['product_price_10'] - df['product_price_1'],\n",
        "        'diff_10_ave_num': lambda df: df['product_num_10'] - df['ave_num'],\n",
        "        'diff_10_ave_price': lambda df: df['product_price_10'] - df['ave_price']\n",
        "    }\n",
        "\n",
        "    # 特徴量生成関数\n",
        "    def apply_feature_operations(df, feature_ops):\n",
        "        for feature_name, operation in feature_ops.items():\n",
        "            df[feature_name] = operation(df)\n",
        "        return df\n",
        "\n",
        "    # 訓練データに特徴量を生成\n",
        "    train_df_copy = train_df.copy()\n",
        "    train_df_gen = apply_feature_operations(train_df_copy, feature_operations)\n",
        "\n",
        "    # テストデータに特徴量を生成\n",
        "    test_df_copy = test_df.copy()\n",
        "    test_df_gen = apply_feature_operations(test_df_copy, feature_operations)\n",
        "\n",
        "    return train_df_gen, test_df_gen\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import jpholiday\n",
        "\n",
        "def add_calendar_features(train_df, test_df, start_date='2018-01-01', end_date='2019-12-31', predict_year_month=(2019, 12)):\n",
        "    \"\"\"\n",
        "    訓練データとテストデータにカレンダー情報を追加する関数。\n",
        "\n",
        "    Parameters:\n",
        "    - train_df (pd.DataFrame): 訓練データフレーム。\n",
        "    - test_df (pd.DataFrame): テストデータフレーム。\n",
        "    - start_date (str, optional): カレンダー情報の開始日。デフォルトは '2018-01-01'。\n",
        "    - end_date (str, optional): カレンダー情報の終了日。デフォルトは '2019-12-31'。\n",
        "    - predict_year_month (tuple, optional): テストデータ用の年と月。デフォルトは (2019, 12)。\n",
        "\n",
        "    Returns:\n",
        "    - train_df_cal (pd.DataFrame): カレンダー情報が追加された訓練データフレーム。\n",
        "    - test_df_cal (pd.DataFrame): カレンダー情報が追加されたテストデータフレーム。\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 休日判定関数の定義\n",
        "    def isHoliday(date):\n",
        "        if jpholiday.is_holiday(date):\n",
        "            return 1\n",
        "        elif date.weekday() >= 5:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # 2. 日数と休日数の集計\n",
        "    start_datetime = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end_datetime = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    delta = end_datetime - start_datetime\n",
        "    date_list = [start_datetime + timedelta(days=i) for i in range(delta.days + 1)]\n",
        "\n",
        "    # 日付データフレームの作成\n",
        "    all_date_df = pd.DataFrame({'datetime': date_list})\n",
        "    all_date_df['is_holiday'] = all_date_df['datetime'].apply(isHoliday).astype(int)\n",
        "    all_date_df['year'] = all_date_df['datetime'].dt.year\n",
        "    all_date_df['month'] = all_date_df['datetime'].dt.month\n",
        "\n",
        "    # (year, month)ごとに日数と休日数を集計\n",
        "    day_holiday_num_df = all_date_df.groupby(['year', 'month']).agg(\n",
        "        day_each_month=('datetime', 'count'),\n",
        "        holiday_each_month=('is_holiday', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # day_holiday_monthを計算\n",
        "    day_holiday_num_df['day_holiday_month'] = day_holiday_num_df['day_each_month'] * day_holiday_num_df['holiday_each_month']\n",
        "    day_holiday_num_df.set_index(['year', 'month'], inplace=True)\n",
        "\n",
        "    # 2018年12月 〜　2019年10月\n",
        "    day_holiday_num_df_12_10 = day_holiday_num_df.loc[(2018, 12): (2019, 10)]\n",
        "    day_holiday_num_df_12_10 = day_holiday_num_df_12_10.reset_index().set_index('month')\n",
        "\n",
        "    # 2019年12月 (予測用)\n",
        "    day_holiday_num_df_12 = day_holiday_num_df.loc[(2019, 12)]\n",
        "    day_holiday_num_df_12 = pd.DataFrame(day_holiday_num_df_12)\n",
        "\n",
        "    # カレンダー情報のマッピング用 DataFrameを準備\n",
        "    calendar_train_df = day_holiday_num_df_12_10.reset_index().rename(columns={'month': 'month_target'})\n",
        "\n",
        "    # 3. 訓練データにカレンダー情報をマージ\n",
        "    train_df_cal = train_df.merge(\n",
        "        calendar_train_df[['month_target', 'day_each_month', 'holiday_each_month', 'day_holiday_month']],\n",
        "        on='month_target',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # 4. テストデータにカレンダー情報を追加\n",
        "    test_df_cal = test_df.merge(\n",
        "        calendar_train_df[['month_target', 'day_each_month', 'holiday_each_month', 'day_holiday_month']],\n",
        "        on='month_target',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # テストデータ用の2019年12月のカレンダー情報を追加\n",
        "    test_df_cal['day_each_month'] = day_holiday_num_df_12.loc['day_each_month', (2019, 12)]\n",
        "    test_df_cal['holiday_each_month'] = day_holiday_num_df_12.loc['holiday_each_month', (2019, 12)]\n",
        "    test_df_cal['day_holiday_month'] = day_holiday_num_df_12.loc['day_holiday_month', (2019, 12)]\n",
        "\n",
        "    return train_df_cal, test_df_cal\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 特徴量生成用関数\n",
        "def generate_grouped_features(df, group_cols, target_col, feature_suffixes, agg_func='sum'):\n",
        "    \"\"\"\n",
        "    グループごとの集計を計算し、新しい特徴量として追加する関数\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): 対象のデータフレーム\n",
        "    - group_cols (list): グループ化するカラム名のリスト\n",
        "    - target_col (str): 集計を計算する対象のカラム名\n",
        "    - feature_suffixes (list): 生成する特徴量のサフィックス\n",
        "    - agg_func (str): 使用する集計関数（'sum' または 'mean'）\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: 新しい特徴量が追加されたデータフレーム\n",
        "    \"\"\"\n",
        "    for suffix in feature_suffixes:\n",
        "        feature_name = f\"{agg_func}_num_{suffix}\"\n",
        "        product_num_col = f\"{target_col}_{suffix}\"\n",
        "        df[feature_name] = df.groupby(group_cols)[product_num_col].transform(agg_func)\n",
        "    return df\n",
        "\n",
        "# 差分特徴量生成用関数\n",
        "def generate_difference_features(df, base_feature, comparison_features, new_feature_names):\n",
        "    \"\"\"\n",
        "    基準特徴量と比較特徴量の差分を計算し、新しい特徴量として追加する関数\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): 対象のデータフレーム\n",
        "    - base_feature (str): 差分の基準となる特徴量名\n",
        "    - comparison_features (list): 比較対象となる特徴量名のリスト\n",
        "    - new_feature_names (list): 新しく生成する差分特徴量名のリスト\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: 新しい差分特徴量が追加されたデータフレーム\n",
        "    \"\"\"\n",
        "    for comp_feat, new_feat in zip(comparison_features, new_feature_names):\n",
        "        df[new_feat] = df[base_feature] - df[comp_feat]\n",
        "    return df\n",
        "\n",
        "# 特徴量生成のメイン関数\n",
        "def feature_engineering(train_df, test_df, group_cols, target_col, feature_suffixes, diff_features):\n",
        "    \"\"\"\n",
        "    特徴量生成および差分計算を行うメイン関数\n",
        "\n",
        "    Parameters:\n",
        "    - train_df (pd.DataFrame): トレーニング用データフレーム\n",
        "    - test_df (pd.DataFrame): テスト用データフレーム\n",
        "    - group_cols (list): グループ化するカラム名のリスト\n",
        "    - target_col (str): 集計を計算する対象のカラム名\n",
        "    - feature_suffixes (list): 生成する特徴量のサフィックス\n",
        "    - diff_features (dict): 差分特徴量の生成情報（基準特徴量と比較特徴量）\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, pd.DataFrame: 特徴量が追加されたトレーニング用およびテスト用データフレーム\n",
        "    \"\"\"\n",
        "    # 1. 特徴量生成（合計）\n",
        "    train_df = generate_grouped_features(train_df, group_cols, target_col, feature_suffixes, agg_func='sum')\n",
        "    test_df = generate_grouped_features(test_df, group_cols, target_col, feature_suffixes, agg_func='sum')\n",
        "\n",
        "    # 2. 差分特徴量生成\n",
        "    base_feature = 'sum_num_10'  # 基準となる特徴量\n",
        "    comparison_features = ['sum_num_9', 'sum_num_8']\n",
        "    new_diff_feature_names = ['ave_num_10_9', 'ave_num_10_8']\n",
        "\n",
        "    train_df = generate_difference_features(train_df, base_feature, comparison_features, new_diff_feature_names)\n",
        "    test_df = generate_difference_features(test_df, base_feature, comparison_features, new_diff_feature_names)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_sales_uptrend_flag(train_df, test_df, flag_num=15, test_product_ids=[2900075]):\n",
        "    \"\"\"\n",
        "    訓練データとテストデータに対して、売上上昇傾向フラグを作成する関数。\n",
        "\n",
        "    Parameters:\n",
        "    - train_df (pd.DataFrame): 訓練データフレーム。\n",
        "    - test_df (pd.DataFrame): テストデータフレーム。\n",
        "    - flag_num (int, optional): 売上増加の閾値。デフォルトは15。\n",
        "    - test_product_ids (list, optional): テストデータでフラグを設定する対象の product_id のリスト。デフォルトは [2900075]。\n",
        "\n",
        "    Returns:\n",
        "    - train_df_up (pd.DataFrame): up_num_flag が追加された訓練データフレーム。\n",
        "    - test_df_updated (pd.DataFrame): up_num_flag が追加されたテストデータフレーム。\n",
        "    \"\"\"\n",
        "\n",
        "    # 訓練データの処理\n",
        "    train_df_copy = train_df.copy()\n",
        "\n",
        "    # diff_12_10 を計算\n",
        "    train_df_copy['diff_12_10'] = train_df_copy['product_num_12'] - train_df_copy['product_num_10']\n",
        "\n",
        "    # diff_12_10 > flag_num のインデックスを取得\n",
        "    up_num_index = train_df_copy.loc[train_df_copy['diff_12_10'] > flag_num].index\n",
        "\n",
        "    # up_num_flag を0で初期化\n",
        "    train_df_copy.insert(2, 'up_num_flag', 0)\n",
        "\n",
        "    # up_num_flag を1に設定\n",
        "    train_df_copy.loc[up_num_index, 'up_num_flag'] = 1\n",
        "\n",
        "    # diff_12_10 を削除\n",
        "    train_df_up = train_df_copy.drop('diff_12_10', axis=1)\n",
        "\n",
        "    # フラグが設定された商品の数を表示\n",
        "    num_flags = train_df_up['up_num_flag'].sum()\n",
        "    print(f\"訓練データで up_num_flag が1に設定された商品の数: {num_flags}\")\n",
        "\n",
        "    # テストデータの処理\n",
        "    test_df_updated = test_df.copy()\n",
        "\n",
        "    # up_num_flag を0で初期化\n",
        "    test_df_updated.insert(2, 'up_num_flag', 0)\n",
        "\n",
        "    # 指定された product_id に対して up_num_flag を1に設定\n",
        "    for pid in test_product_ids:\n",
        "        test_df_updated.loc[test_df_updated['product_id'] == pid, 'up_num_flag'] = 1\n",
        "\n",
        "    # 指定された product_id で product_num_10 が0以下の場合、up_num_flag を0に設定\n",
        "    for pid in test_product_ids:\n",
        "        condition = (test_df_updated['product_id'] == pid) & (test_df_updated['product_num_10'] <= 0)\n",
        "        test_df_updated.loc[condition, 'up_num_flag'] = 0\n",
        "\n",
        "    # 特定の product_id の一部カラムを表示（デバッグ用）\n",
        "    for pid in test_product_ids:\n",
        "        display_df = test_df_updated.loc[test_df_updated['product_id'] == pid, ['product_id', 'product_num_10', 'up_num_flag']]\n",
        "        print(f\"product_id == {pid} のデータ:\")\n",
        "        print(display_df)\n",
        "\n",
        "    return train_df_up, test_df_updated\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def split_train_validation_and_sort_test(train_df_up,\n",
        "                                         test_df_feats5,\n",
        "                                         validation_main_flag=1,\n",
        "                                         validation_month_target=12,\n",
        "                                         sort_columns=['product_id', 'store_id']):\n",
        "    \"\"\"\n",
        "    訓練データを検証用データと訓練用データに分割し、テストデータをソートする関数。\n",
        "\n",
        "    Parameters:\n",
        "    - train_df_up (pd.DataFrame): 売上上昇傾向フラグが追加された訓練データフレーム。\n",
        "    - test_df_feats5 (pd.DataFrame): 前処理が完了したテストデータフレーム。\n",
        "    - validation_main_flag (int, optional): 検証用データの main_flag 条件。デフォルトは 1。\n",
        "    - validation_month_target (int, optional): 検証用データの month_target 条件。デフォルトは 12。\n",
        "    - sort_columns (list, optional): テストデータのソートに使用するカラム。デフォルトは ['product_id', 'store_id']。\n",
        "\n",
        "    Returns:\n",
        "    - validation_df (pd.DataFrame): 検証用データフレーム。\n",
        "    - train_df (pd.DataFrame): 訓練用データフレーム。\n",
        "    - sorted_test_df (pd.DataFrame): ソートされたテストデータフレーム。\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 訓練データの分割\n",
        "    validation_df = train_df_up.loc[\n",
        "        (train_df_up['main_flag'] == validation_main_flag) &\n",
        "        (train_df_up['month_target'] == validation_month_target)\n",
        "    ].copy()\n",
        "\n",
        "    train_df = train_df_up.loc[\n",
        "        (train_df_up['main_flag'] != validation_main_flag) |\n",
        "        (train_df_up['month_target'] != validation_month_target)\n",
        "    ].copy()\n",
        "\n",
        "    # 2. テストデータのソート\n",
        "    sorted_test_df = test_df_feats5.sort_values(sort_columns).copy()\n",
        "\n",
        "    return validation_df, train_df, sorted_test_df\n"
      ],
      "metadata": {
        "id": "lwwe1U2A7S2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4bd6e67-e838-4af3-f805-d4da3d1407ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *"
      ],
      "metadata": {
        "id": "HqqfIol2Do85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データ読み込み\n",
        "sales_history_df, item_categories_df, category_names_df, test_df = load_data(data_dir)\n",
        "\n",
        "# データの前処理\n",
        "join_data_df = preprocess_data(sales_history_df, item_categories_df, category_names_df)\n",
        "\n",
        "# 特徴量生成\n",
        "join_data_df_final = generate_features(join_data_df)\n",
        "\n",
        "catarog_df = complete_catalog(join_data_df_final)\n",
        "\n",
        "catarog_df_copy = catarog_df.copy()\n",
        "catarog_fill = fill_missing_values(catarog_df_copy, test_df)\n",
        "\n",
        "catarog_copy = catarog_fill.copy()\n",
        "catarog_copy_feats1 = fill_features(catarog_copy)\n",
        "\n",
        "catarog_feats1_copy = catarog_copy_feats1.copy()\n",
        "train_df, test_df = generate_sliding_window_datasets(catarog_feats1_copy)\n",
        "\n",
        "# 訓練データとテストデータに対して特徴量を生成\n",
        "train_df_gen, test_df_gen = generate_trend_features(train_df, test_df)\n",
        "\n",
        "train_df_gen_copy = train_df_gen.copy()\n",
        "test_df_gen_copy = test_df_gen.copy()\n",
        "\n",
        "# 3. カレンダー情報を追加\n",
        "train_df_cal, test_df_cal = add_calendar_features(\n",
        "    train_df=train_df_gen_copy,\n",
        "    test_df=test_df_gen_copy,\n",
        "    start_date='2018-01-01',\n",
        "    end_date='2019-12-31',\n",
        "    predict_year_month=(2019, 12)\n",
        ")\n",
        "\n",
        "train_df_cal_copy = train_df_cal.copy()\n",
        "test_df_cal_copy = test_df_cal.copy()\n",
        "\n",
        "# 使用例\n",
        "group_cols = ['month_target', 'product_id']\n",
        "target_col = 'product_num'\n",
        "feature_suffixes = [10, 9, 8]\n",
        "diff_features = {\n",
        "    'ave_num_10_9': ('ave_num_10', 'ave_num_9'),\n",
        "    'ave_num_10_8': ('ave_num_10', 'ave_num_8'),\n",
        "}\n",
        "\n",
        "# 特徴量生成を関数で実行\n",
        "train_df_feats5, test_df_feats5 = feature_engineering(\n",
        "    train_df_cal_copy,\n",
        "    test_df_cal_copy,\n",
        "    group_cols,\n",
        "    target_col,\n",
        "    feature_suffixes,\n",
        "    diff_features=None  # 今回は差分特徴量は手動で追加\n",
        ")\n",
        "\n",
        "train_df_feats5_copy = train_df_feats5.copy()\n",
        "test_df_feats5_copy = test_df_feats5.copy()\n",
        "\n",
        "train_df_up, test_df_up = create_sales_uptrend_flag(train_df_feats5_copy,test_df_feats5_copy)\n",
        "\n",
        "train_df_up_copy = train_df_up.copy()\n",
        "test_df_up_copy = test_df_up.copy()\n",
        "\n",
        "# 2. データ分割およびソート関数を使用して validation_df, train_df, sorted_test_df を作成\n",
        "validation_df, train_df, sorted_test_df = split_train_validation_and_sort_test(\n",
        "    train_df_up=train_df_up_copy,\n",
        "    test_df_feats5=test_df_up_copy,\n",
        "    validation_main_flag=1,\n",
        "    validation_month_target=12,  # 必要に応じて変更\n",
        "    sort_columns=['product_id', 'store_id']  # 必要に応じて変更\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrzrKraD-llu",
        "outputId": "6a8714d8-358a-474c-93a5-296e1a0a8bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データを読み込んでいます...\n",
            "データの読み込みが完了しました。\n",
            "データの前処理を開始します...\n",
            "データの前処理が完了しました。\n",
            "特徴量を生成しています...\n",
            "特徴量の生成が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:00<00:00, 25.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "欠損値を補完しています...\n",
            "欠損値の補完が完了しました。\n",
            "追加の特徴量を生成しています...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/9040 [00:00<?, ?it/s]/content/utils.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.5555555555555556' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['product_id'] == product_id, 'product_ave_num'] = product_ave_num[product_id]\n",
            "/content/utils.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '356.1309209210279' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['product_id'] == product_id, 'product_ave_price'] = product_ave_price[product_id]\n",
            "100%|██████████| 9040/9040 [00:15<00:00, 581.98it/s]\n",
            "  0%|          | 0/26 [00:00<?, ?it/s]/content/utils.py:122: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.27570912782180385' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['category_id'] == category_id, 'category_ave_num'] = category_ave_num[category_id]\n",
            "/content/utils.py:123: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '145.2676186426149' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['category_id'] == category_id, 'category_ave_price'] = category_ave_price[category_id]\n",
            "100%|██████████| 26/26 [00:00<00:00, 422.20it/s]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]/content/utils.py:132: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5327534191472245' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['store_id'] == store_id, 'store_ave_num'] = store_ave_num[store_id]\n",
            "/content/utils.py:133: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '399.31767009111445' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  join_data_df10.loc[join_data_df10['store_id'] == store_id, 'store_ave_price'] = store_ave_price[store_id]\n",
            "100%|██████████| 18/18 [00:00<00:00, 343.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "追加の特徴量生成が完了しました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "スライディングウィンドウ適用中: 100%|██████████| 11/11 [00:02<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データで up_num_flag が1に設定された商品の数: 2017\n",
            "product_id == 2900075 のデータ:\n",
            "      product_id  product_num_10  up_num_flag\n",
            "159      2900075            34.0            1\n",
            "329      2900075             0.0            0\n",
            "499      2900075            39.0            1\n",
            "669      2900075             4.0            1\n",
            "839      2900075            11.0            1\n",
            "1009     2900075             5.0            1\n",
            "1179     2900075            15.0            1\n",
            "1349     2900075            23.0            1\n",
            "1519     2900075             8.0            1\n",
            "1689     2900075            78.0            1\n",
            "1859     2900075            21.0            1\n",
            "2029     2900075            10.0            1\n",
            "2199     2900075            14.0            1\n",
            "2369     2900075            65.0            1\n",
            "2539     2900075             8.0            1\n",
            "2709     2900075             4.0            1\n",
            "2879     2900075             8.0            1\n",
            "3049     2900075             6.0            1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "845rzGOHF7ek"
      },
      "source": [
        "# 保存"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdata_dir = \"/content/drive/MyDrive/マナビDX/PBL01/演習03（機械学習）/DXQuest_PBL01/Data/FeatureData/\""
      ],
      "metadata": {
        "id": "yuORh6b3F7ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_df.to_csv(fdata_dir + 'train_12_df.csv')\n",
        "train_df.to_csv(fdata_dir + 'train_1_11_df.csv')\n",
        "sorted_test_df.to_csv(fdata_dir + 'test_data_df.csv')"
      ],
      "metadata": {
        "id": "37gs81V0F7ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIpRAmnDDLqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
